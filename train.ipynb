{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# ByT5 모델 학습 및 추론 Notebook\n",
    "----------\n",
    "# 실험 코드 구성\n",
    "#### 1. 라이브러리 Import\n",
    "#### 2. 사용 함수 정의\n",
    "#### 3. 데이터 확인 및 전처리\n",
    "#### 4. 데이터 전처리 \n",
    "#### 5. 학습\n",
    "#### 6. 평가 \n",
    "#### 7. 추론\n",
    "#### 8. 제출 파일 생성 \n",
    "\n",
    "----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55b84c5",
   "metadata": {},
   "source": [
    "# 1. 라이브러리 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import comet_ml\n",
    "# from transformers.integrations import CometCallback\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import datetime\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a551b5",
   "metadata": {},
   "source": [
    "### +) GPU 설정 및 시드 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14730c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac8052",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b8b505",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comet-init",
   "metadata": {},
   "source": [
    "### +) 시각화 툴(Comet ML) 실험 초기화\n",
    "\n",
    "실험 기록에 Comet ML을 이용  \n",
    "(WandaDB를 사용하시는 분들은 대체 하시면 될 듯 합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comet-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# today = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "# experiment = comet_ml.start(\n",
    "#     api_key=\"-\",  # 실제 API key 입력\n",
    "#     project_name=\"unreadable-korean\",\n",
    "#     workspace=\"kong\"\n",
    "# )\n",
    "# experiment.set_name(f\"run_{today}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ad583f",
   "metadata": {},
   "source": [
    "-----\n",
    "# 2. 사용 함수 정의\n",
    "\n",
    "## 2-1. 입력 텍스트 길이 확인 함수 정의\n",
    "데이터셋의 입력 텍스트에 대한 분석 진행\n",
    "- 입력 텍스트 토큰 길이를 확인 \n",
    "- 분포와 최대 길이를 시각화해서 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63260bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_max_length(tokenizer, df):\n",
    "    all_texts = df[\"input\"]\n",
    "    lengths = []\n",
    "    for text in all_texts:\n",
    "        token_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "        lengths.append(len(token_ids))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=50, alpha=0.75, edgecolor='black')\n",
    "    plt.title('토큰 길이 분포 확인')\n",
    "    plt.xlabel('Token Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "char-f1-func",
   "metadata": {},
   "source": [
    "## 2-2. 커스텀 char F1 계산 함수 정의\n",
    "\n",
    "예측 문자열과 정답 문자열 간의 문자 단위 F1 score를 계산하는 함수  \n",
    "- 해당 평가 산식을 지표로 이용해서 훈련을 진행\n",
    "- 대회 평가에 적합한 성능 기대"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "char-f1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_char_f1_for_lists(pred_texts, ref_texts):\n",
    "    f1_scores = []\n",
    "    for pred_text, ref_text in zip(pred_texts, ref_texts):\n",
    "        if not isinstance(pred_text, str):\n",
    "            pred_text = \"\" if pd.isna(pred_text) else str(pred_text)\n",
    "        if not isinstance(ref_text, str):\n",
    "            ref_text = \"\" if pd.isna(ref_text) else str(ref_text)\n",
    "        pred_text = pred_text.strip()\n",
    "        ref_text = ref_text.strip()\n",
    "        num_same = sum(\n",
    "            1 for i in range(min(len(pred_text), len(ref_text))) if pred_text[i] == ref_text[i]\n",
    "        )\n",
    "        if len(pred_text) == 0 or len(ref_text) == 0:\n",
    "            f1 = 0.0\n",
    "        else:\n",
    "            precision = num_same / len(pred_text)\n",
    "            recall = num_same / len(ref_text)\n",
    "            if (precision + recall) == 0:\n",
    "                f1 = 0.0\n",
    "            else:\n",
    "                f1 = 2 * precision * recall / (precision + recall)\n",
    "        f1_scores.append(f1)\n",
    "    avg_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0.0\n",
    "    return {\"char_f1\": avg_f1}\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    preds = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "    # -100 토큰을 패딩 토큰 ID로 대체 (디코딩 오류 방지)\n",
    "    labels = [\n",
    "        [token if token != -100 else tokenizer.pad_token_id for token in label_seq]\n",
    "        for label_seq in labels\n",
    "    ]\n",
    "    preds = np.clip(preds, 0, tokenizer.vocab_size - 1)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = compute_char_f1_for_lists(decoded_preds, decoded_labels)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocess-chunk",
   "metadata": {},
   "source": [
    "## 2-3. 전처리 함수 정의 (Chunk 단위 분할 적용)\n",
    "\n",
    "Chunk란 데이터를 잘라서 사용하는 것.  \n",
    "- 문맥의 손실의 위험성이 발생함.  \n",
    "\n",
    "- 그럼에도 불구하고 Chunk를 사용한 이유는?\n",
    "\n",
    "1. 데이터 상의 입력 텍스트가 상당히 길다.  \n",
    "2. 사용한 모델인 ByT5가 데이터 자체를 Byte로 만들어 사용하다 보니, 가뜩이나 긴 입력 데이터가 더 길어짐\n",
    "3. 하지만 훈련데이터를 짤라서 버릴 수 없다. 주어진 환경에서 최대한 많은 문장을 훈련하고자, 문맥이 잘리는 것을 감안\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocess-chunk-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련시 사용\n",
    "def preprocess_with_chunking(examples, tokenizer, max_length=1024, chunk_overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_length,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    new_inputs = []\n",
    "    new_outputs = []\n",
    "    for input_text, output_text in zip(examples['input'], examples['output']):\n",
    "        if not isinstance(input_text, str):\n",
    "            input_text = str(input_text)\n",
    "        chunks = splitter.split_text(input_text)\n",
    "        for chunk in chunks:\n",
    "            new_inputs.append(chunk)\n",
    "            new_outputs.append(output_text)\n",
    "    tokenized_inputs = tokenizer(\n",
    "        new_inputs,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"longest\"\n",
    "    )\n",
    "    tokenized_labels = tokenizer(\n",
    "        new_outputs,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"longest\"\n",
    "    ).input_ids\n",
    "    tokenized_inputs[\"labels\"] = tokenized_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "langchain-predict",
   "metadata": {},
   "source": [
    "## 2-4. 추론 시 데이터 처리 함수 정의 (Chunk 단위 분할 적용)\n",
    "\n",
    "\n",
    "** 훈련시의 데이터 전처리를 동일하게 적용해서 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "langchain-predict-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "#추론시 사용\n",
    "def predict_with_chunking(example, tokenizer, model, \n",
    "                           chunk_size=1024, chunk_overlap=100, \n",
    "                           gen_max_tokens=1024):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = splitter.split_text(example['input'])\n",
    "    outputs = []\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(\n",
    "            chunk,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=chunk_size,\n",
    "            truncation=False,\n",
    "            padding=\"longest\"\n",
    "        ).to(model.device)\n",
    "        result = model.generate(\n",
    "            **inputs,\n",
    "            num_beams=4,\n",
    "            repetition_penalty=2.3,\n",
    "            max_new_tokens = len(tokenizer.encode(example['input'])) + 100,\n",
    "            do_sample=False\n",
    "        )\n",
    "        out_text = tokenizer.decode(result[0], skip_special_tokens=True)\n",
    "        outputs.append(out_text)\n",
    "    final_output = \" \".join(outputs)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07d6be0",
   "metadata": {},
   "source": [
    "-------\n",
    "# 3. 모델 및 토크나이저 로드 \n",
    "\n",
    "- ByT5에서는 글자 자체를 인코딩해서 사용하기 때문에 토크나이저가 필요 없음\n",
    "- 여기서 AutoTokenizer Class는 토크나이저가 아닌 인코딩의 역할을 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d6ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/byt5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main-section",
   "metadata": {},
   "source": [
    "------\n",
    "# 4. 데이터 확인 및 전처리\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befcac30",
   "metadata": {},
   "source": [
    "## 4-1. 데이터 로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbce876",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "sample_submission_df = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2e18f5",
   "metadata": {},
   "source": [
    "## 4-2. Byte화 된 데이터 분포 확인\n",
    "- Model의 입력데이터 형태에 따라 달라짐\n",
    "- ByT5의 입력데이터는 문장을 Byte화 후 사용\n",
    "- 해당 문장의 Byte화 된 길이를 확인 후에 Chunk방식의 입력 데이터 전처리 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b23ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train 입력 데이터 길이 분포\n",
    "check_max_length(tokenizer, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36490f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test 입력 데이터 길이 분포\n",
    "check_max_length(tokenizer, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51e15f8",
   "metadata": {},
   "source": [
    "## 4-3. 데이터 전처리\n",
    "- train_max_len(1024) : ByT5의 모델 최대 입력 길이 참조 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e80f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_max_len = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22275c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- 데이터셋 준비 (청크 기반 전처리 적용) ----------------------\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.005, random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "# 청크 단위 전처리 함수 적용\n",
    "train_dataset = train_dataset.map(lambda examples: preprocess_with_chunking(examples, tokenizer ,max_length=train_max_len, chunk_overlap=100), batched=True, remove_columns=train_dataset.column_names)\n",
    "val_dataset   = val_dataset.map(lambda examples: preprocess_with_chunking(examples, tokenizer, max_length=train_max_len, chunk_overlap=100), batched=True, remove_columns=val_dataset.column_names)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59cf659",
   "metadata": {},
   "source": [
    "-------\n",
    "# 5. 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb4ffea",
   "metadata": {},
   "source": [
    "## 5-1. 인자 설정(초기화)\n",
    "- ByT5은 Transformer Encoder-Decoder 기반의 seq2seq 모델\n",
    "- 그러므로 Seq2SeqTrainer 사용이 적합하다고 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945138cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2SeqTrainig\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    save_total_limit=3,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=100, #크게 잡고 early_stopping, load_best_model 적용\n",
    "    logging_dir=\"./logs\",\n",
    "    # report_to=\"comet_ml\",\n",
    "    bf16=True,\n",
    "    bf16_full_eval=True,\n",
    "    \n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=train_max_len,\n",
    "    generation_num_beams=1,\n",
    "    \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_char_f1\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "# Seq2SeqTrainer \n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "            # CometCallback(), \n",
    "            EarlyStoppingCallback(early_stopping_patience=100)\n",
    "            ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0242ce2e",
   "metadata": {},
   "source": [
    "## 5-2. 훈련 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539761fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train()\n",
    "#experiment.end() # 실험 기록 종료(WandaDB로 교체 가능)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2cd3a1",
   "metadata": {},
   "source": [
    "----\n",
    "## 6. 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63506c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(\"모델 Evaluate Results:\")\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b6f633",
   "metadata": {},
   "source": [
    "------\n",
    "# 7. 추론\n",
    "\n",
    "훈련시의 데이터 전처리(Chunk 분할 방식)를 동일하게 적용해서 추론\n",
    "- 적용 이유: 모델의 훈련 방식과 최대한 비슷하게 추론해야 훈련된 모델의 최고의 성능이 나올 것이라고 판단  \n",
    "- (슬프게도 실제 평가 시에 성능이 많이 높아지지는 않음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best 모델 가져오기\n",
    "checkpoints = [cp for cp in os.listdir(\"./checkpoints\") if cp.startswith(\"checkpoint-\")]\n",
    "sorted_checkpoint = sorted(checkpoints, key=lambda x: int(x.split('-')[-1]))\n",
    "checkpoint = sorted_checkpoint[0]\n",
    "model_path = os.path.join(\"./checkpoints\", checkpoint)\n",
    "\n",
    "print(f\"predict model: {checkpoint}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "tqdm.pandas()\n",
    "\n",
    "def predict(example):\n",
    "    return predict_with_chunking(\n",
    "        example,\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        chunk_size=train_max_len,       \n",
    "        chunk_overlap=100,               \n",
    "        gen_max_tokens=4800     \n",
    "    )\n",
    "\n",
    "sample_submission_df['output'] = test_df['input'].progress_apply(\n",
    "    lambda x: predict({'input': x})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b4135",
   "metadata": {},
   "source": [
    "----\n",
    "## 8. 제출 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f479c005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 내용 제출 파일 저장\n",
    "today = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_path = f\"output/byt5_{checkpoint}_{today}.csv\"\n",
    "sample_submission_df.to_csv(output_path, index=False)\n",
    "print(f\"저장 완료 {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unreadable_ko",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
